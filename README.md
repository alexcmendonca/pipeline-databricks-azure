# Databricks e Data Factory | Criando e orquestrando pipelines com Azure

## üí°Objetivos
Desenvolver um pipeline de engenharia de dados, usando recursos da ferramenta de Cloud utilizada pela Microsoft Azure, agendando a execu√ß√£o dos notebooks no ambiente do Azure Databricks, utilizando ferramentas como PySpark, Airflow, Databricks e requisi√ß√µes de API. Incluindo, utiliza√ß√£o do Data Factory e o Databricks com Python para ler e manipular os dados nas camadas bronze e silver de um Data Lake que ser√° criado no Azure.

###### Imagem 1: Etapas do pipeline
<img src="/img/etapas-pipeline.png">

Para execu√ß√£o de todas as etapas desse pipeline, foi utilizado o Trello para organizar todas as atividades necess√°rias para concluir este projeto.

###### Imagem 2: Projeto Trello 
<img src="/img/trello-projeto.png">

## üñ•Ô∏èDesafios do Projeto
Para configurar os recursos na Azure e estruturar nosso Data Lake para armazenar e organizar os dados, foi utilizada a tecnologia Data Lake Gen 2 da Azure. Inicialmente, configurando o registro de aplicativo e atribuindo permiss√µes adequadas para gerenciar o acesso aos dados no Data Lake.

###### Imagem 3: Integrando Databricks com GitHub
<img src="/img/img-playlist.png">

Em seguida, realizada configura√ß√£o no Databricks para executar as transforma√ß√µes nos dados, aproveitando os recursos da Azure. Cria√ß√£o do workspace Databricks dentro da pr√≥pria Azure, garantindo acesso √† ferramenta e configurando-a conforme necess√°rio. Conex√£o estabelecida entre o Databricks e o Data Lake para acesso e manipula√ß√£o dos dados.

Para a transforma√ß√£o dos dados, inicialmente os salvando na camada bronze, aplicando transforma√ß√µes iniciais com base nas necessidades das equipes que utilizar√£o esses dados. Em seguida, aplicando transforma√ß√£o novamente e os salvando na camada silver, convertendo cada campo do JSON em uma coluna individual e removendo informa√ß√µes desnecess√°rias sobre as caracter√≠sticas dos im√≥veis.

Posteriormente, foi criado e configurado o Data Factory para agendar e executar os notebooks de forma programada, garantindo a integridade e efici√™ncia pipeline para os novos lotes de dados de im√≥veis que chegar√£o ao Data Lake. Configurando tamb√©m o acesso workspace do Databricks, permitindo que o Data Factory se conecte e utilize os recursos do Databricks para executar as atividades planejadas no pipeline.

Realizado testes essenciais para garantir a execu√ß√£o correta do pipeline e identificar poss√≠veis erros. Ao final, configura√ß√£o de um gatilho de execu√ß√£o, j√° que novos dados de im√≥veis ser√£o periodicamente adicionados ao Data Lake, com uma frequ√™ncia de atualiza√ß√£o de uma vez por hora. Isso garantir√° que o pipeline seja executado de acordo com esse intervalo de tempo.

###### Imagem 4: Cria√ß√£o de um pipeline do Data Factory
<img src="/img/pipeline-datafactory.png">

## üìÑConhecimentos Desenvolvidos
|Atividades|Realizadas |
|----------|-----------|
| Criar uma conta na Azure | Definir um or√ßamento e configurar alerta de gastos |
| Navegar no Trello do projeto do curso | Criar um grupo de recursos |
| Criar uma conta de armazenamento na Azure | Estruturar um Data Lake |
| Entender o que s√£o as camada inbound, bronze e silver | Configurar um registro de aplicativo |
| Atribuir permiss√µes em um Data Lake | Utilizar o IAM e o Gerenciamento de ACL da Azure |
| Criar o workspace do Databricks na Azure | Conectar o Databricks ao GitHub |
| Criar um cluster no Databricks | Ativar e encerrar a execu√ß√£o de um cluster |
| Lidar com poss√≠veis erros na cria√ß√£o do cluster | Conectar o Databricks ao Data Lake |
| Utilizar Python no Databricks | Aplicar transforma√ß√µes nos dados |
| Salvar dados no formato Delta | Armazenar os dados nas diferentes camadas do Data Lake |
| Criar e configurar o Data Factory na plataforma da Azure | Navegar pela interface do Data Factory |
| Entender as diferen√ßas do Data Factory para o Airflow | Integrar o Data Factory com o GitHub |
| Desenvolver um pipeline no Data Factory | Conectar o Databricks ao Data Factory |
| Depurar o pipeline | Criar um gatilho de execu√ß√£o |
| Publicar e colocar o pipeline em produ√ß√£o | Deletar os recursos da Azure |

##  üóÇÔ∏èOrganiza√ß√£o dos Arquivos
* Notebooks Jupyter | Databricks
    - Projeto Spotify  - Parte 1 at√© 3: Explora√ß√£o e tratamento dos dados, sele√ß√£o de dados, configura√ß√£o do cluster, implementa√ß√£o do algoritmo Kmeans, c√°lculo de dist√¢ncias. Utilizando tamb√©m a API do Spotify, chegamos √† cria√ß√£o da playlist.

    - Desafio Projeto Spotify - Manipula√ß√£o e transforma√ß√£o eficientes de dados e arquivos utilizando o Apache Spark em conjunto com o formato Parquet. Inclu√≠ndo convers√£o de arquivos CSV para o formato Parquet, garantindo maior desempenho e efici√™ncia no processamento e armazenamento dos dados.

* Data - Conjunto de dados do Spotify, obtido do Kaggle, uma plataforma vital na comunidade de dados, √© uma fonte valiosa de informa√ß√µes para an√°lise e explora√ß√£o no campo da ci√™ncia de dados.

## üéûÔ∏èImagens do Projeto

###### Imagem 4: Monitorando execu√ß√£o do pipeline no Data Factory
<img src="/img/img-grafico-dispersao.png">

###### Imagem 4: Monitorando execu√ß√£o do pipeline no Databricks
<img src="/img/img-api-spotify.png">

###### Imagem 4: Notebook Databricks
<img src="/img/img-databricks.png">

## üîçRefer√™ncias
- [Alura](https://www.alura.com.br/)
