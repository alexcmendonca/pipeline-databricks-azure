# Databricks e Data Factory | Criando e orquestrando pipelines com Azure

## üí°Objetivos
Desenvolver um pipeline de engenharia de dados, usando recursos da ferramenta de Cloud utilizada pela Microsoft Azure, agendando a execu√ß√£o dos notebooks no ambiente do Azure Databricks, utilizando ferramentas como PySpark, Databricks e requisi√ß√µes de API. Incluindo, utiliza√ß√£o do Data Factory e o Databricks com Python para ler e manipular os dados nas camadas bronze e silver de um Data Lake que ser√° criado no Azure.

###### Imagem 1: Etapas do pipeline
<img src="/img/etapas-pipeline.png">

Para execu√ß√£o de todas as etapas desse pipeline, foi utilizado o Trello para organizar todas as atividades necess√°rias para concluir este projeto.

###### Imagem 2: Projeto Trello 
<img src="/img/trello-projeto.png">

## üñ•Ô∏èDesafios do Projeto
Para configurar os recursos na Azure e estruturar nosso Data Lake para armazenar e organizar os dados, foi utilizada a tecnologia Data Lake Gen 2 da Azure. Inicialmente, configurando o registro de aplicativo e atribuindo permiss√µes adequadas para gerenciar o acesso aos dados no Data Lake.

###### Imagem 3: Integrando Databricks com GitHub
<img src="/img/img-databricks-github.png">

Em seguida, realizada configura√ß√£o no Databricks para executar as transforma√ß√µes nos dados, aproveitando os recursos da Azure. Cria√ß√£o do workspace Databricks dentro da pr√≥pria Azure, garantindo acesso √† ferramenta e configurando-a conforme necess√°rio. Conex√£o estabelecida entre o Databricks e o Data Lake para acesso e manipula√ß√£o dos dados.

Para a transforma√ß√£o dos dados, inicialmente os salvando na camada bronze, aplicando transforma√ß√µes iniciais com base nas necessidades das equipes que utilizar√£o esses dados. Em seguida, aplicando transforma√ß√£o novamente e os salvando na camada silver, convertendo cada campo do JSON em uma coluna individual e removendo informa√ß√µes desnecess√°rias sobre as caracter√≠sticas dos im√≥veis.

Posteriormente, foi criado e configurado o Data Factory para agendar e executar os notebooks de forma programada, garantindo a integridade e efici√™ncia pipeline para os novos lotes de dados de im√≥veis que chegar√£o ao Data Lake. Configurando tamb√©m o acesso workspace do Databricks, permitindo que o Data Factory se conecte e utilize os recursos do Databricks para executar as atividades planejadas no pipeline.

Realizado testes essenciais para garantir a execu√ß√£o correta do pipeline e identificar poss√≠veis erros. Ao final, configura√ß√£o de um gatilho de execu√ß√£o, j√° que novos dados de im√≥veis ser√£o periodicamente adicionados ao Data Lake, com uma frequ√™ncia de atualiza√ß√£o de uma vez por hora. Isso garantir√° que o pipeline seja executado de acordo com esse intervalo de tempo.

###### Imagem 4: Cria√ß√£o de um pipeline do Data Factory
<img src="/img/pipeline-datafactory.png">

## üìÑConhecimentos Desenvolvidos
|Atividades|Realizadas |
|----------|-----------|
| Criar uma conta na Azure | Definir um or√ßamento e configurar alerta de gastos |
| Navegar no Trello do projeto do curso | Criar um grupo de recursos |
| Criar uma conta de armazenamento na Azure | Estruturar um Data Lake |
| Entender o que s√£o as camada inbound, bronze e silver | Configurar um registro de aplicativo |
| Atribuir permiss√µes em um Data Lake | Utilizar o IAM e o Gerenciamento de ACL da Azure |
| Criar o workspace do Databricks na Azure | Conectar o Databricks ao GitHub |
| Criar um cluster no Databricks | Ativar e encerrar a execu√ß√£o de um cluster |
| Lidar com poss√≠veis erros na cria√ß√£o do cluster | Conectar o Databricks ao Data Lake |
| Utilizar Python no Databricks | Aplicar transforma√ß√µes nos dados |
| Salvar dados no formato Delta | Armazenar os dados nas diferentes camadas do Data Lake |
| Criar e configurar o Data Factory na plataforma da Azure | Navegar pela interface do Data Factory |
| Entender as diferen√ßas do Data Factory para o Airflow | Integrar o Data Factory com o GitHub |
| Desenvolver um pipeline no Data Factory | Conectar o Databricks ao Data Factory |
| Depurar o pipeline | Criar um gatilho de execu√ß√£o |
| Publicar e colocar o pipeline em produ√ß√£o | Deletar os recursos da Azure |

##  üóÇÔ∏èOrganiza√ß√£o dos Arquivos Databricks e Data Factory
* Notebooks | Databricks
    - Transforma√ß√µes nos dados e salvando-os em um arquivo Delta Lake no reposit√≥rio Azure Data Lake.

* Factory | Azure Data Factory
    - Arquivos de configura√ß√£o carregados no reposit√≥rio GitHub ap√≥s conex√£o Data Factory

* Databricks-pcm
    - Arquivos de configura√ß√£o da conex√£o do Databricks ao Data Factory

* Pipeline
    - Arquivos de configura√ß√£o do pipeline

* Trigger
    - Arquivos de configura√ß√£o do gatilho de execu√ß√£o

## üéûÔ∏èImagens do Projeto

###### Imagem 5: Monitorando execu√ß√£o do pipeline no Data Factory
<img src="/img/monitorando-execucao-pipeline.png">

###### Imagem 6: Monitorando execu√ß√£o do pipeline no Databricks / Job runs
<img src="/img/monitorando-execucao-pipeline-databricks.png">

###### Imagem 7: Tela inicial do est√∫dio Azure Data Factory
<img src="/img/estudio-azure-data-factory.png">

## üîçRefer√™ncias
- [Alura](https://www.alura.com.br/)